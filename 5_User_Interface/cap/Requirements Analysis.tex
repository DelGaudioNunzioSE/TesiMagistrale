\clearpage
\section{Requirements Analysis}
Unfortunately, it was not possible to distribute questionnaires regarding 
the requirements for a code detection interface. However, it wasn't difficult 
to find successful portals to draw inspiration from. Methods for detecting 
natural text on the web tend to have a clean interface, display a probability 
percentage of detection, and show the sections of the text that most likely 
indicate that the text is generated by an LLM. This last request was not 
feasible due to the different detection method used for code 
(code embedding does not allow us to understand which sections of 
the code lead to the conclusion that it was generated by an LLM).

A requirement set during development was the need to create a web-based 
interface. In this way, it would not be complicated, if desired, to make 
the code detector easily accessible to any user via the web by setting up 
an appropriately equipped server. Given the request to make this interface 
not only a simple detector but also a validator of the methods presented 
and datasets proposed, it was crucial to make the detection interface easily 
and immediately accessible, clearly separated from other sections.

Given the large number of datasets and methods presented, it was necessary 
to visually differentiate each dedicated page, using colors (for datasets) 
and different page layouts for the detection methods. In fact, each detection 
method had to provide several possible operations, such as: training a model or 
loading the weights of a previously trained model, easily launching training, 
allowing methods based on perplexity to calculate the best threshold or set it 
manually before launching any test. Additionally, for each dataset, an automated 
dataset balancing method, an automatic split for training, validation, and test 
sets, and a standard format for each dataset had to be provided.