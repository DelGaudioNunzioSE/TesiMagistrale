\section{Dataset Evaluation Criteria}



All datasets made publicly available by the scientific 
community will be carefully analysed and compared. 
The goal is to identify the most suitable dataset—either 
directly or through integration, for the evaluation of code 
detection methods under consistent conditions. Each dataset 
will be assessed based on the following criteria:

\begin{enumerate}
    \item \textbf{Total number of code samples available:} 
    a larger number of samples generally improves statistical 
    robustness during evaluation and model training.
    
    \item \textbf{LLMs used to generate synthetic code:} 
    datasets that include code generated by multiple LLMs are better 
    suited for evaluating a method’s generalization capability.
    \begin{enumerate}
        \item \textbf{Number of LLMs:} 
        to avoid a method from focusing too much on 
        stylistic features of a specific LLM, it is essential 
        to have codes generated by different LLMs.
        \item \textbf{Actual degree of use in contemporary times:}
        while it is desirable for detection methods to have 
        long-term viability, it is undeniable that a detector 
        effective on current LLMs (such as GPT-4 \cite{openai2023gpt4}) is more useful 
        than one targeting outdated models (such as GPT-3 \cite{brown2020language}).
        \item \textbf{Actual types diversity among LLMs:}
        it is also necessary to analyse the diversity of the models 
        themselves. For instance, it is more informative to include 
        in the same dataset code produced by both an open-source LLM 
        like CodeLlama\cite{roziere2023code} 
        and a commercial one like GPT-3 \cite{brown2020language}, rather than, for 
        example, code from GPT-3 \cite{brown2020language} 
        and Codex \cite{chen2021codex} (which is itself based on GPT-3).
    \end{enumerate}
    
    \item \textbf{Code diversity:} 
    ensuring diversity in the code is essential to understand 
    how generalizable a method is to the overall code domain, 
    or whether it is specialized in the stylistic patterns of 
    specific LLMs \textit{(a particularly relevant issue for methods relying 
    on stylistic features of the code)}.
        \begin{enumerate}
        \item \textbf{Different programming languages:}
        while Python is currently the most relevant language 
        in the field of artificial code generation (indeed, many 
        datasets primarily consist of Python code 
        such as MBPP \cite{austin2021program}), 
        it is valuable 
        to evaluate detection methods across as many programming 
        languages as possible. At the very least, it is worth 
        assessing different detection methods on different languages, 
        possibly recommending specific approaches depending 
        on the language being analysed.
        \item \textbf{Different types of code:}
        although language differences are important, 
        they are not sufficient. Python, for example, 
        can be used both as an object-oriented and a procedural 
        language. Understanding the type of code being analysed 
        can aid both in training and in evaluating a detector.
        \item \textbf{Code size:}
        code length is a key parameter, as the ease of identifying 
        the origin of the code may vary depending on its size. 
        It is worth noting that in certain contexts, such as homework 
        assignments, short code snippets are extremely common.
        \item \textbf{Code context:}
        it is useful to include examples 
        of human-written code from both competitive settings 
        (such as Leetcode dataset \cite{xia2025leetcodedataset}) 
        and open-source project environments 
        (such as CodeSearchNet \cite{husain2019codesearchnet}), 
        in order to evaluate detectors across different real-world contexts.
    \end{enumerate}

    \item \textbf{Validity information:} 
    this includes whether the dataset provides information on the 
    validity of human-written code, the exact prompts used to generate 
    LLM code, or whether code quality and correctness have been validated.
    \begin{enumerate}
        \item \textbf{Generation prompts:}
        it may be useful to evaluate the different 
        prompts used to generate the code, as it is 
        well known that varying the prompt can lead to 
        significantly different outputs from an LLM.
        \item \textbf{Source of human-written code:}
        many datasets are based on collections of human-written 
        code that are later used to generate LLM outputs. Knowing 
        the source of this human code ensures the reliability 
        and correctness of the dataset.
        \item \textbf{Code quality:}
        performing tests such as verifying whether 
        both human and LLM-generated code actually 
        works is essential. \textit{There is little value in 
        analysing whether non-functional code was 
        produced by an LLM or not.}
        \item \textbf{Paper reliability:} 
        this point is important for preprinted paper, for which 
        we can't rely on the scientific community.
    \end{enumerate}

\end{enumerate}


Although all these are evaluation parameters, it is natural that some 
are more relevant than others: knowing the source of the human-written 
code remains more important than simply including a few additional LLMs 
in the code generation process.
