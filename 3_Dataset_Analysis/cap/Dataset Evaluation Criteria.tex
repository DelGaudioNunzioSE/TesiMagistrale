\section{Dataset Evaluation Criteria}



All datasets made publicly available by the scientific 
community will be carefully analysed and compared. 
The goal is to identify the most suitable dataset—either 
directly or through integration, for the evaluation of code 
detection methods under consistent conditions. Each dataset 
will be assessed based on the following criteria:

\begin{enumerate}
    \item \textbf{Total number of code samples available:} 
    a larger number of samples generally improves statistical 
    robustness during evaluation and model training.
    
    \item \textbf{LLMs used to generate synthetic code:} 
    datasets that include code generated by multiple LLMs are better 
    suited for evaluating a method’s generalization capability.
    \begin{enumerate}
        \item \textbf{Number of LLMs:} 
        to avoid a method from focusing too much on 
        stylistic features of a specific LLM, it is essential 
        to have codes generated by different LLMs.
        \item \textbf{Actual degree of use in contemporary times:}
        while it is desirable for detection methods to have 
        long-term viability, it is undeniable that a detector 
        effective on current LLMs (such as GPT-5 \cite{openai2025gpt5}) is more useful 
        than one targeting outdated models (such as GPT-3 \cite{brown2020language}).
        \item \textbf{Actual types diversity among LLMs:}
        it is also necessary to analyse the diversity of the models 
        themselves. For instance, it is more informative to include 
        in the same dataset code produced by both an open-source LLM 
        like CodeLlama\cite{roziere2023code} 
        and a commercial one like GPT-3 \cite{brown2020language}, rather than, for 
        example, code from GPT-3 \cite{brown2020language} 
        and Codex \cite{chen2021codex} (which is itself based on GPT-3).
    \end{enumerate}
    
    \item \textbf{Code diversity:} 
    ensuring diversity in the code is essential to understand 
    how generalizable a method is to the overall code domain, 
    or whether it is specialized in the stylistic patterns of 
    specific LLMs \textit{(a particularly relevant issue for methods relying 
    on stylistic features of the code)}.
        \begin{enumerate}
        \item \textbf{Different programming languages:}
        while Python is currently the most relevant language 
        in the field of artificial code generation (indeed, many 
        datasets primarily consist of Python code 
        such as MBPP \cite{austin2021program}), 
        it is valuable 
        to evaluate detection methods across as many programming 
        languages as possible. At the very least, it is worth 
        assessing different detection methods on different languages, 
        possibly recommending specific approaches depending 
        on the language being analysed.
        \item \textbf{Different programming paradigm:}
        although language differences are important, 
        they are not sufficient. Python, for example, 
        can be used both as an object-oriented and a procedural 
        language. Understanding the programming paradigm of code being analysed 
        can aid both in training and in evaluating a detector.
        \item \textbf{Code size:}
        code length is a key parameter, as the ease of identifying 
        the origin of the code may vary depending on its size. 
        It is worth noting that in certain contexts, such as homework 
        assignments, short code snippets are extremely common.
        \item \textbf{Code context:}
        it is useful to include examples 
        of human-written code and prompt from different settings: 
        \begin{enumerate}
            \item Competitive programming: short code intended to assess the programmer’s reasoning ability (such as Leetcode \cite{xia2025leetcodedataset}).
            \item Open source: code typically heavily commented and maintained by multiple programmers with different programming styles (such as CodeSearchNet \cite{husain2019codesearchnet}).
            \item Academic corpus: Code designed to evaluate an LLM’s ability to generate code (such as HumanEval \cite{chen2021codex}).
            \item Q\&A: Code from support-request platforms such as \href{https://stackoverflow.com/}{Stack Overflow}.
            \item industrial code: Code that is typically closed or under paid licenses.
            \item Test code: Code intended to run tests on a larger program.
        \end{enumerate}
    \end{enumerate}

    \item \textbf{Validity information:} 
    this includes whether the dataset provides information on the 
    validity of human-written code, the exact prompts used to generate 
    LLM code, or whether code quality and correctness have been validated.
    \begin{enumerate}
        \item \textbf{Generation prompts:}
        it may be useful to evaluate the different 
        prompts used to generate the code, as it is 
        well known that varying the prompt can lead to 
        significantly different outputs from an LLM.
        \item \textbf{Source of human-written code:}
        many datasets are based on collections of human-written 
        code that are later used to generate LLM outputs. Knowing 
        the source of this human code ensures the reliability 
        and correctness of the dataset.
        \item \textbf{Code quality:}
        performing tests such as verifying whether 
        both human and LLM-generated code actually 
        works is essential. \textit{There is little value in 
        analysing whether non-functional code was 
        produced by an LLM or not.}
        \item \textbf{Paper reliability:} 
        This point is important for preprinted papers, 
        for which reliance on the scientific community is 
        not possible.
    \end{enumerate}

\end{enumerate}


Although all these are evaluation parameters, it is natural that some 
are more relevant than others: knowing the source of the human-written 
code remains more important than simply including a few additional LLMs 
in the code generation process.
