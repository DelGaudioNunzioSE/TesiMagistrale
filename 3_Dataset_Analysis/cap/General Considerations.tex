\clearpage

\section{Considerations}
The variety of available data is undeniable. 
Nevertheless, during the analysis, an issue emerged that is rarely 
addressed in the reviewed papers: the correctness of 
LLM-generated code. It is self-evident that anyone using 
LLM-generated code would, before committing it, 
most certainly run at least one test to verify that the 
code works. Moreover, for our purposes 
(\hyperref[sec:Motivations Behind LLM-Generated Code Detection]{Section 1.2}), 
whether a student or candidate submits non-functional 
code, it makes little difference whether it was written by them or 
by an LLM. Therefore, the consideration that every detection method 
should be evaluated specifically on \textbf{functional} LLM-generated code, 
rather than on LLM code in general, proves useful in preventing a 
significant gap between the reported metrics and the actual user 
experience. Such a gap may arise if detection methods exhibit a 
classification bias due to the incorrect behaviour of the code. 
It is worth noting that while many works perform syntax or runtime 
checks, none explicitly mention functional testing.

\subsection{Test framework}
\label{section:Test framework}

To evaluate the potential correlation between code correctness 
(i.e., functional vs. non-functional code) and the detection of 
LLM-generated code, a small framework was developed and published on
\href{https://github.com/DelGaudioNunzioSE/Code-LLMsTester.git}{GitHub}.
 This framework is designed 
to assess the correctness of code submitted as a solution to a 
known problem.

Importantly, the LLM is explicitly instructed to generate 
\textbf{challenging} test cases, in order to prevent it from producing 
overly simplistic tests that the provided code could easily pass.

The reason for not asking the LLM to directly classify the code as 
correct or incorrect is supported by the findings of the paper 
\textit{``Do LLMs generate test oracles that capture the actual or 
the expected program behaviour''}~\cite{konstantinou2024llms}, which states: 
\textit{``LLMs are more effective at generating test oracles rather than classifying them.''}

The framework has been developed starting from the project \textbf{KodCode}\cite{xu2025kodcode}, 
available on \href{https://github.com/KodCode-AI/kodcode/tree/main}{GitHub}. 
KodCode aims to generate code from prompts in order to train LLMs on 
synthetic code. 
However, the project underwent huge changes, leaving very 
little of the original code (like the \textit{.jsonl} use).


\subsubsection{Development}
Initially, the idea was to slightly modify the \texttt{KodCode} 
framework so as not to require a large language model (LLM) 
to generate both code and tests, but only the tests. 
However, during the development and analysis of the 
framework, it became evident that much more substantial 
changes and additions were necessary.

First and foremost, it was necessary to develop a 
method for preprocessing and preparing a dataset of 
pre-written code. Subsequently, the entire generation 
phase had to be adapted to meet the new objectives.

After initial testing, it became clear that an LLM 
of moderate size (14B), a necessary compromise due to 
hardware limitations, was not sufficiently capable of 
understanding the problem and the code to generate 
effective tests. In many cases, the tests produced 
were incorrect and would have incorrectly flagged 
correct code as faulty.

As a result, it became necessary to change the 
approach: rather than relying solely on the LLM’s 
test-generation capabilities, we opted to use 
functioning reference code that solved the same problem. 
Since the dataset proposed by Pan et al.\ includes both 
LLM-generated code and corresponding human-written code 
(the latter being reliably correct), we focused on that 
dataset.

The new strategy required generating inputs consistent 
with both the problem statement and the function signatures. 
The LLM's task was thus limited to generating such inputs 
and writing code to verify whether both the human-written 
and LLM-generated implementations would produce the same 
outputs for those inputs.

Several challenges emerged from this new approach, some 
of which are outlined below:

\begin{enumerate}
    \item \textbf{Lack of Imports}: Human-written code in 
    the dataset did not include any imports. We therefore 
    introduced a mechanism to automatically insert the most 
    common imports, as the LLM proved ineffective at 
    consistently adding all necessary ones.
    
    \item \textbf{Code Structure Variability}: Not all 
    solutions were simple functions—some were classes, 
    and others were standalone Python scripts. For 
    class-based solutions, the prompts were adapted to 
    enable the LLM to instantiate objects correctly and 
    generate the necessary tests. For script-based code, 
    we chose to discard such examples, as converting 
    them into testable functions could have introduced 
    errors attributable to the LLM's transformation 
    process rather than to the code itself.
    
    \item \textbf{Name Collisions}: In many cases, 
    the human-written and LLM-generated code used 
    identical class or function names. To address this, 
    we implemented an algorithmic renaming strategy to 
    avoid such conflicts.
\end{enumerate}

Subsequently, it became necessary to develop an 
entire sub-framework responsible for effectively 
generating the test files. These files needed to 
contain both the human-written and LLM-generated 
functions, as well as the tests produced by the LLM.

The sub-framework was also tasked with executing 
multiple tests in parallel, reporting the result of 
each individual execution, and generating a new dataset 
based on the previous one, but enriched with additional 
information regarding the outcome of the tests.

Despite the significant effort invested in 
developing the framework, it was decided to maintain, 
at least in this phase, certain limitations—potentially 
addressable in future work, such as compatibility with the 
Python programming language only.