\subsection{Suh et al}
The goal of this paper is both to evaluate detectors 
originally designed for natural language (which, as 
expected, proved to be ineffective), and to propose 
alternative approaches, including fine-tuning 
LLMs, an approach that was effective only for code 
belonging to the same training domain. To achieve 
these results, the authors developed a large dataset, 
which should therefore be rightly considered as a 
separate contribution.


\begin{itemize}
    \item \textbf{GPT (GPT-3.5)}: Improved variant of GPT-3 with 
    fine-tuning and alignment, optimized for conversation and for 
    generating both text and code.
    \item \textbf{GPT-4}: Successor of GPT-3.5 with enhanced 
    capabilities in reasoning, context understanding, robustness, 
    and multilingual/technical generation.
    \item \textbf{Gemini Pro (Gemini 1.0)}: Model from the Gemini 
    family (Google/DeepMind), “Pro” version of release 1.0. ACM Digital Library.
    Gemini is a multimodal model with capabilities in text, images, video, audio, etc.
    \item \textbf{Starcoder2-Insgruct (15B)}: “Instruct” version of the 
    StarCoder2 model with 15 billion parameters, 
    featuring a transparent and permissive pipeline.
\end{itemize}

%%%%
%%%%

\subsubsection*{Strengths}
\begin{itemize}
    \item Use different domain dataset.
    \begin{itemize}
        \item \textbf{MBPP} \cite{austin2021mbpp}: A dataset of 974 Python programming tasks solvable by entry-level programmers.
        \item Each task includes: a natural language description, function signature, canonical implementation, and 3 unit tests.
        \item \textbf{humaneEval-x} \cite{chen2021codex}: A benchmark released by OpenAI with 164 hand-written programming tasks in Python.
        \item \textbf{Code Search Net} \cite{codesearchnet2019}: A benchmark corpus 
        for code retrieval tasks. Models are given a natural language query and must retrieve relevant code functions across multiple programming languages (e.g. Python, Java, etc.).
    \end{itemize}
\end{itemize}


%%%%
%%%%
\subsubsection*{Weaknesses}
\begin{itemize}
    \item \textbf{Unfortunately, the data they used is no longer available 
through the official portals, which, at the time of writing, 
are no longer accessible. As a result, this dataset is 
effectively unusable.}.
\end{itemize}



\subsubsection*{Code Quality}
In order to preserve quality in the LLMs code source:
\begin{itemize}
    \item \textit{"We also removed code snippets that contain syntax errors"}
\end{itemize}


%%%%
%%%%
\subsubsection*{Final evaluation}






\expandafter\def\csname SuhHumanCode\endcsname{3,700}
\expandafter\def\csname SuhLLMCode\endcsname{29,500}
\expandafter\def\csname SuhNumLLMs\endcsname{4 \textit{chatgpt, gpt-4, Gemini pro, stracider2-instruct 15B}}
\expandafter\def\csname SuhLLMDiversity\endcsname{3 \textit{gpt, gemini, stracider}}
\expandafter\def\csname SuhCurrentUse\endcsname{Average year: 2024}
\expandafter\def\csname SuhLanguages\endcsname{java, C++, python}
\expandafter\def\csname SuhCodeTypes\endcsname{unspecified}
\expandafter\def\csname SuhCodeSize\endcsname{1\textsuperscript{st} percentile:  words,\newline 3\textsuperscript{rd} percentile:  words}
\expandafter\def\csname SuhCodeContext\endcsname{Both}
\expandafter\def\csname SuhPrompts\endcsname{not in the paper}
\expandafter\def\csname SuhSources\endcsname{MBPP, humaneEval-x, Code Search Net}
\expandafter\def\csname SuhCodeQuality\endcsname{Sintax quality}
\expandafter\def\csname SuhReliability\endcsname{Peer-review paper}

\evaluationTable{Suh}



