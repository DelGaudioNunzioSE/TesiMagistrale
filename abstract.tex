\section*{Abstract}
\subsection*{Description of the problem addressed}
\noindent 
As research has progressed, generative AI and, more specifically, 
LLMs have become common and widespread tools. With their increasing ability 
to generate text resembling that written by humans, tools have emerged to 
determine whether a text was generated by an LLM or written by a human. 
This thesis aims to address the detection of LLM-generated content in a 
specific domain: code generation.


\subsection*{Contextualization of the project within the contemporary technical landscape}
\noindent 
Although the field of natural-text detection for LLM-generated content seems to have achieved good 
results (e.g., DetectGPT or GPTZero), the problem of detecting LLM-generated code has not 
led to equally positive outcomes. The reason is that perplexity-based detection 
(used by DetectGPT) appears to be extremely ineffective due to the inherently more 
deterministic nature of code compared to natural text (code constructs follow standard 
forms, while in natural text there are infinite ways to express the same concept).
In the scientific literature, detection of LLM-generated code is not yet widespread. 
Many published works are in a pre-publication state. Moreover, several report good results 
but do not release the code, datasets, or trained models used. Not all works use the same 
evaluation metrics to compare models, and they rarely compare against methods designed for 
code detection, opting instead to compare with methods for natural-text detection. 
Some works have released datasets for training or testing detection methods, but these 
have not been widely adopted by others.


\subsection*{Personal contribution of the candidate to solving the problem described}
\noindent 
The activities of this thesis were: collecting all datasets useful for training and 
testing detection models; retrieving all publicly available detection methods; creating a 
framework to test the actual functionality of the code samples in the datasets; evaluating 
dataset quality and identifying the best purpose for each; identifying the most relevant 
metrics for evaluating the different detection methods; testing detection methods on the 
selected datasets using the chosen metrics; proposing improvements to the detection methods; 
developing an interface for using the selected detection method; and creating a graphical 
interface to reproduce the presented tests and generate subsets of the datasets used.


\subsection*{Description of the application and experimental content of the paper}
\noindent
Since natural-text detection is simpler than code detection, to ensure realistic results from both 
training and test datasets all natural-language comments were removed. Multiple datasets were 
selected for tests to evaluate different capabilities of the detection method: generalization 
across different generating LLMs, generalization across different programming languages, 
and generalization across different code types (educational code, competitive code, open-source 
project code, etc.). All tests were conducted out of domain (the training dataset differs from 
the test dataset) to show how these detection methods (contrary to what is shown in the 
literature) perform poorly out of domain and to select the method that achieves better 
out-of-domain results. TPR@FPR=10\% was chosen as the main comparison metric. 
The reason behind this decision was the intent to have a method that rarely 
accuses an innocent of having generated LLM code, preferring FN over FP.