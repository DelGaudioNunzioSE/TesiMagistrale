@article{base,
  author    = {Ye, Tianyu and Du, Yujia and Ma, Tong and Wu, Lijun and Zhang, Xinyu and Ji, Shuang and Wang, Wei},
  title     = {Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting},
  year      = {2025},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {39},
  number    = {1},
  pages     = {968--976},
  doi       = {10.1609/aaai.v39i1.3208}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION

@article{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and et al.},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and et al.},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{anthropic2023claude,
  title={Claude: The Anthropic Language Model},
  author={Anthropic},
  year={2023},
  howpublished={\url{https://www.anthropic.com/index/introducing-claude}}
}

@article{jiang2023mistral,
  title={Introducing Mistral: A High-Performance Language Model},
  author={Jiang, Jianfeng and et al.},
  journal={Mistral AI},
  year={2023},
  howpublished={\url{https://mistral.ai/news/introducing-mistral-7b/}}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and et al.},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and et al.},
  journal={NeurIPS},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/research/gpt-4}}
}

@inproceedings{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and et al.},
  booktitle={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{roziere2023code,
  title={Code LLaMA: Open Foundation Models for Code},
  author={Roziere, Baptiste and et al.},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{rombach2022high,
  title={High-Resolution Image Synthesis with Latent Diffusion Models},
  author={Rombach, Robin and et al.},
  journal={CVPR},
  year={2022}
}

@misc{google2024veo,
  author    = {Google DeepMind},
  title     = {Veo: Advancing Generative Video Models},
  year      = {2024},
  howpublished = {\url{https://deepmind.google/technologies/veo}}
}

@article{warwick2016turing,
  title={Can Machines Think? A Report on Turing Test Experiments at the Royal Society},
  author={Warwick, Kevin and Shah, Huma},
  journal={Journal of Experimental \& Theoretical Artificial Intelligence},
  volume={28},
  number={6},
  pages={989--1007},
  year={2016}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1.1
@article{weizenbaum1966eliza,
  title={ELIZAâ€”a computer program for the study of natural language communication between man and machine},
  author={Weizenbaum, Joseph},
  journal={Communications of the ACM},
  volume={9},
  number={1},
  pages={36--45},
  year={1966}
}

@article{mikolov2013efficient,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Mikolov, Tomas and et al.},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={GloVe: Global Vectors for Word Representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
  booktitle={EMNLP},
  year={2014}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and et al.},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  year={2019}
}

@inproceedings{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and et al.},
  booktitle={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wang2021codet5,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and et al.},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}

@article{nijkamp2022codegen,
  title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and et al.},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{zeng2022codegeex,
  title={CodeGeeX: A Pre-trained Model for Code Generation with Cross-Programming-Language Evaluation},
  author={Zeng, Guolin and et al.},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{roziere2023code,
  title={Code LLaMA: Open Foundation Models for Code},
  author={Roziere, Baptiste and et al.},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1.1.2
@article{backus1957fortran,
  title={The FORTRAN automatic coding system},
  author={Backus, John and et al.},
  journal={Proceedings of the Western Joint Computer Conference},
  year={1957},
  pages={188--198}
}

@misc{stallman1981emacs,
  title={EMACS: The extensible, customizable self-documenting display editor},
  author={Stallman, Richard M.},
  year={1981},
  howpublished={\url{https://www.gnu.org/software/emacs/}}
}

@misc{xslt1999,
  author={W3C},
  title={XSL Transformations (XSLT) Version 1.0},
  year={1999},
  howpublished={\url{https://www.w3.org/TR/xslt}}
}

@inproceedings{zelle1996learning,
  title={Learning to parse database queries using inductive logic programming},
  author={Zelle, John M. and Mooney, Raymond J.},
  booktitle={AAAI/IAAI, Vol. 2},
  year={1996},
  pages={1050--1055}
}

@inproceedings{mooney1997nlidb,
  title={Learning semantic parsers: An important but under-studied application of machine learning},
  author={Mooney, Raymond J.},
  booktitle={Proceedings of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web},
  year={1997}
}

@misc{jinja2docs,
  title={Jinja2 Documentation},
  author={Pallets Projects},
  year={2005},
  howpublished={\url{https://jinja.palletsprojects.com/}}
}

@misc{makoengine,
  title={Mako Templates for Python},
  author={Mike Bayer},
  year={2006},
  howpublished={\url{https://www.makotemplates.org/}}
}

@inproceedings{yin2017syntactic,
  title={A syntactic neural model for general-purpose code generation},
  author={Yin, Pengcheng and Neubig, Graham},
  booktitle={ACL},
  year={2017}
}

@inproceedings{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and et al.},
  booktitle={arXiv preprint arXiv:2107.03374},
  year={2021}
}
