@article{base,
  author    = {Ye, Tianyu and Du, Yujia and Ma, Tong and Wu, Lijun and Zhang, Xinyu and Ji, Shuang and Wang, Wei},
  title     = {Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting},
  year      = {2025},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {39},
  number    = {1},
  pages     = {968--976},
  doi       = {10.1609/aaai.v39i1.3208}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{anthropic2023claude,
  title={Claude: The Anthropic Language Model},
  author={Anthropic},
  year={2023},
  howpublished={\url{https://www.anthropic.com/index/introducing-claude}}
}

@article{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


#ex openai2023gpt4,
@misc{openai2023gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{codesearchnet2019,
  title = {CodeSearchNet: A Benchmark for Code Retrieval},
  author = {Husain, Haseeb and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Gu, Xin and Brockschmidt, Marc and Abbas, Arash and others},
  year = {2019},
  howpublished = {GitHub / dataset},
  note = {Accessed: YYYY-MM-DD, \url{https://github.com/github/CodeSearchNet}}
}

@inproceedings{chen2021codex,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{roziere2023code,
  title={Code LLaMA: Open Foundation Models for Code},
  author={Roziere, Baptiste and et al.},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@misc{google2024veo,
  author    = {Google DeepMind},
  title     = {Veo: Advancing Generative Video Models},
  year      = {2024},
  howpublished = {\url{https://deepmind.google/technologies/veo}}
}

@article{warwick2016turing,
  title={Can Machines Think? A Report on Turing Test Experiments at the Royal Society},
  author={Warwick, Kevin and Shah, Huma},
  journal={Journal of Experimental \& Theoretical Artificial Intelligence},
  volume={28},
  number={6},
  pages={989--1007},
  year={2016}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1.1
@article{weizenbaum1966eliza,
  title={ELIZA—a computer program for the study of natural language communication between man and machine},
  author={Weizenbaum, Joseph},
  journal={Communications of the ACM},
  volume={9},
  number={1},
  pages={36--45},
  year={1966},
  publisher={ACM New York, NY, USA}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas},
  journal={arXiv preprint arXiv:1301.3781},
  volume={3781},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and et al.},
  booktitle={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wang2021codet5,
  title={Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}

@article{nijkamp2022codegen,
  title={Codegen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{chen2021codex,
  title        = {Evaluating Large Language Models Trained on Code},
  author       = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde de Oliveira Pinto, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal      = {arXiv preprint arXiv:2107.03374},
  year         = {2021}
}
@article{zeng2022codegeex,
  title={Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x},
  author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Shen, Lei and Wang, Zihan and Wang, Andi and Li, Yang and others},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={5673--5684},
  year={2023}
}

@article{roziere2023code,
  title={Code LLaMA: Open Foundation Models for Code},
  author={Roziere, Baptiste and et al.},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@online{openai2023gpt35turbo,
  author = {OpenAI},
  title = {GPT-3.5 Turbo model},
  year = {2023},
  url = {https://platform.openai.com/docs/models/gpt-3.5-turbo},
  note = {Accessed: YYYY-MM-DD}
}
@online{openai2023gpt4,
  author = {OpenAI},
  title = {GPT-4},
  year = {2023},
  url = {https://openai.com/research/gpt-4},
  note = {Accessed: YYYY-MM-DD}
}
@techreport{deepmind2024gemini,
  author = {DeepMind / Google},
  title = {Gemini: A Family of Highly Capable Multimodal Models},
  year = {2024},
  url = {https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf},
  note = {Accessed: YYYY-MM-DD}
}
@article{lozhkov2024starcoder2,
  author = {A. Lozhkov and coauthors},
  title = {StarCoder2 and The Stack v2: The Next Generation},
  year = {2024},
  journal = {arXiv preprint arXiv:2402.19173},
  url = {https://arxiv.org/pdf/2402.19173}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1.1.2
@article{backus1957fortran,
  title={The FORTRAN automatic coding system},
  author={Backus, John and et al.},
  journal={Proceedings of the Western Joint Computer Conference},
  year={1957},
  pages={188--198}
}

@misc{stallman1981emacs,
  title={EMACS: The extensible, customizable self-documenting display editor},
  author={Stallman, Richard M.},
  year={1981},
  howpublished={\url{https://www.gnu.org/software/emacs/}}
}

@misc{xslt1999,
  author={W3C},
  title={XSL Transformations (XSLT) Version 1.0},
  year={1999},
  howpublished={\url{https://www.w3.org/TR/xslt}}
}

@inproceedings{zelle1996learning,
  title={Learning to parse database queries using inductive logic programming},
  author={Zelle, John M. and Mooney, Raymond J.},
  booktitle={AAAI/IAAI, Vol. 2},
  year={1996},
  pages={1050--1055}
}

@inproceedings{mooney1997nlidb,
  title={Learning semantic parsers: An important but under-studied application of machine learning},
  author={Mooney, Raymond J.},
  booktitle={Proceedings of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web},
  year={1997}
}

@misc{jinja2docs,
  title={Jinja2 Documentation},
  author={Pallets Projects},
  year={2005},
  howpublished={\url{https://jinja.palletsprojects.com/}}
}

@misc{makoengine,
  title={Mako Templates for Python},
  author={Mike Bayer},
  year={2006},
  howpublished={\url{https://www.makotemplates.org/}}
}

@inproceedings{yin2017syntactic,
  title={A syntactic neural model for general-purpose code generation},
  author={Yin, Pengcheng and Neubig, Graham},
  booktitle={ACL},
  year={2017}
}

@inproceedings{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and et al.},
  booktitle={arXiv preprint arXiv:2107.03374},
  year={2021}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1.1.3
@article{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}

@misc{tabnine2019,
  author={TabNine},
  title={TabNine: Autocomplete AI},
  year={2019},
  howpublished={\url{https://www.tabnine.com/}}
}

@article{husain2019codesearchnet,
  title={Codesearchnet challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}

@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}


#ex austin2021mbpp
@article{austin2021mbpp,
  title = {Program Synthesis with Large Language Models},
  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
  year = {2021},
  journal = {arXiv preprint arXiv:2108.07732},
  url = {https://arxiv.org/abs/2108.07732}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}


# hendrycks2021apps
@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}


@article{lu2021codexglue,
  title={Codexglue: A machine learning benchmark dataset for code understanding and generation},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  journal={arXiv preprint arXiv:2102.04664},
  year={2021}
}


# ex li2022alphacode
@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}


# ex xu2022polycoder
@inproceedings{xu2022systematic,
  title={A systematic evaluation of large language models of code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
  booktitle={Proceedings of the 6th ACM SIGPLAN international symposium on machine programming},
  pages={1--10},
  year={2022}
}

#ex google2024gemini
@article{AlphaCode_2,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={Google DeepMind},
  year={2023},
  howpublished={\url{https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf}}
}


%% image
@article{hui2024qwen2,
  title={Qwen2. 5-coder technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Degradazione del codice 
% ex krishna2023disappearing,
@article{shumailov2023curse,
  title={The curse of recursion: Training on generated data makes models forget},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  journal={arXiv preprint arXiv:2305.17493},
  year={2023}
}

% Copiright
@jurisdiction{DoeVGitHub2024,
  title        = {J. Doe 1, et al. v. GitHub, Inc., et al.},
  number       = {22-cv-06823-JST},
  court        = {United States District Court for the Northern District of California},
  date         = {2024-01-03},
  note         = {Order Granting in Part and Denying in Part Motion to Dismiss, ECF No. 195},
  url          = {https://law.justia.com/cases/federal/district-courts/california/candce/4:2022cv06823/403220/195/},
  urldate      = {2024-05-21}
}

% Security
@inproceedings{perry2022users,
  title={Do users write more insecure code with AI assistants?},
  author={Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
  booktitle={Proceedings of the 2023 ACM SIGSAC conference on computer and communications security},
  pages={2785--2799},
  year={2023}
}

% studenti
@article{Jost2024LLM,
AUTHOR = {Jošt, Gregor and Taneski, Viktor and Karakatič, Sašo},
TITLE = {The Impact of Large Language Models on Programming Education and Student Learning Outcomes},
JOURNAL = {Applied Sciences},
VOLUME = {14},
YEAR = {2024},
NUMBER = {10},
ARTICLE-NUMBER = {4115},
URL = {https://www.mdpi.com/2076-3417/14/10/4115},
ISSN = {2076-3417},
ABSTRACT = {Recent advancements in Large Language Models (LLMs) like ChatGPT and Copilot have led to their integration into various educational domains, including software development education. Regular use of LLMs in the learning process is still not well-researched; thus, this paper intends to fill this gap. The paper explores the nuanced impact of informal LLM usage on undergraduate students’ learning outcomes in software development education, focusing on React applications. We carefully designed an experiment involving thirty-two participants over ten weeks where we examined unrestricted but not specifically encouraged LLM use and their correlation with student performance. Our results reveal a significant negative correlation between increased LLM reliance for critical thinking-intensive tasks such as code generation and debugging and lower final grades. Furthermore, a downward trend in final grades is observed with increased average LLM use across all tasks. However, the correlation between the use of LLMs for seeking additional explanations and final grades was not as strong, indicating that LLMs may serve better as a supplementary learning tool. These findings highlight the importance of balancing LLM integration with the cultivation of independent problem-solving skills in programming education.},
DOI = {10.3390/app14104115}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1.3
@article{mitchell2023detectgpt,
  title={Detectgpt: Zero-shot machine-generated text detection using probability curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={24950--24962},
  year={2023},
  organization={PMLR}
}


@inproceedings{paek2024detection,
  title={Detection of LLM-Generated Java Code Using Discretized Nested Bigrams},
  author={Paek, Timothy and Mohan, Chilukuri},
  booktitle={International Conference on Computational Science and Computational Intelligence},
  pages={118--132},
  year={2024},
  organization={Springer}
}

@techreport{GPTZeroMethodology2023,
  author       = {Tian, Edward and Cui, Alexander and the GPTZero Team},
  title        = {GPTZero's AI Detection Technology},
  institution  = {GPTZero, Inc.},
  year         = {2023},
  url          = {https://gptzero.me/technology},
}


@article{krishna2023paraphrasing,
  title={Paraphrasing is All You Need for Detecting AI-Generated Text},
  author={Krishna, Kalpesh and Efrat, Avia and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2306.04636},
  year={2023}
}

%%

@article{ye2023uncovering,
  title={Uncovering llm-generated code: A zero-shot synthetic code detector via code rewriting},
  author={Ye, Tong and Du, Yangkai and Ma, Tengfei and Wu, Lingfei and Zhang, Xuhong and Ji, Shouling and Wang, Wenhai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={1},
  pages={968--976},
  year={2025}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}



@article{xu2025codevision,
  title={Codevision: Detecting llm-generated code using 2d token probability maps and vision models},
  author={Xu, Zhenyu and Sheng, Victor S},
  journal={arXiv preprint arXiv:2501.03288},
  year={2025}
}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Dataset non per LLM

# Leetcodedataset
@article{xia2025leetcodedataset,
  title={Leetcodedataset: A temporal dataset for robust evaluation and efficient training of code llms},
  author={Xia, Yunhui and Shen, Wei and Wang, Yan and Liu, Jason Klein and Sun, Huifeng and Wu, Siyue and Hu, Jian and Xu, Xiaolong},
  journal={arXiv preprint arXiv:2504.14655},
  year={2025}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% PAPER ANALIZZATI %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









% DATASET
%-------------------------------

%[1] CoDet_M4 ->[dataset] Orel - preprint
@article{orel2025codet, 
  title={CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings},
  author={Orel, Daniil and Azizov, Dilshod and Nakov, Preslav},
  journal={arXiv preprint arXiv:2503.13733},
  year={2025}
}

% Pan et al, <--- citato in CoDet
@inproceedings{pan2024assessing,
  title={Assessing ai detectors in identifying ai-generated code: Implications for education},
  author={Pan, Wei Hung and Chok, Ming Jie and Wong, Jonathan Leong Shan and Shin, Yung Xin and Poon, Yeong Shian and Yang, Zhou and Chong, Chun Yong and Lo, David and Lim, Mei Kuan},
  booktitle={Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
  pages={1--11},
  year={2024}
}


% Codesearchnet, <--- citato in CoDet
@article{husain2019codesearchnet,
  title={Codesearchnet challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}


% sito, <--- citato in CoDet
@online{CodeforcesKaggle,
  author    = {Yeoh Yun Siang Geremie},
  title     = {Codeforces Code Dataset},
  year      = {2023},
  url       = {https://www.kaggle.com/datasets/yeoyunsianggeremie/codeforces-code-dataset},
  urldate   = {2025-07-07},
  note      = {Accessed on Kaggle},
}


@misc{openai_gpt4o_2024,
  author       = {{OpenAI}},
  title        = {Hello GPT-4o},
  year         = {2024},
  month        = {may},
  howpublished = {\url{https://openai.com/index/hello-gpt-4o/}},
  note         = {Accesso effettuato il: 24-07-2024}
}


@misc{meta_llama3_1_2024,
  author       = {{Meta AI}},
  title        = {Introducing Meta Llama 3.1: Our most capable, open models to date},
  year         = {2024},
  month        = {jul},
  howpublished = {\url{https://ai.meta.com/blog/meta-llama-3-1/}},
  note         = {Accesso effettuato il: 24-07-2024}
}

@misc{qwen_codeqwen1.5_blog_2024,
  author       = {{Qwen Team}},
  title        = {CodeQwen1.5: An Open-source Large Language Model Series for Code},
  year         = {2024},
  month        = {feb},
  howpublished = {\url{https://qwenlm.github.io/blog/codeqwen1.5/}},
  note         = {Accesso effettuato il: 24-07-2024}
}


@misc{ntqa_nxcode_dataloop_2024,
  author       = {{ntqa}},
  title        = {ntqa/NxCode-CQ-7B-orpo},
  year         = {2024},
  month        = {may},
  howpublished = {\url{https://dataloop.ai/library/model/ntqai_nxcode-cq-7b-orpo/}},
  note         = {Model page on the Dataloop AI Library. Updated on May 19, 2024. Accesso effettuato il: 24-07-2024}
}


%%%%%%%%%%%%%%%%%%

%[2] AIGCodeSe -> [dataset] Demirok - preprint
@article{demirok2024aigcodeset,
  title={AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection},
  author={Demirok, Basak and Kutlu, Mucahid},
  journal={arXiv preprint arXiv:2412.16594},
  year={2024}
}

% CodeLlama‑34B
@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

% Codestral
@misc{mistral-codestral-2024,
  author       = {{Mistral AI Team}},
  title        = {Codestral: First Open‑Weight Generative AI Model for Code},
  howpublished = {\url{https://mistral.ai/news/codestral/}},
  year         = {2024},
  month        = may,
  day          = {29},
  note         = {Released under Mistral AI Non‑Production License; accessed 2025‑07‑09}
}



% Gemini 1.5
@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}


% dataset originale
@inproceedings{puri2021codenet,
  author       = {Ruchir Puri and David S. Kung and Geert Janssen and Wei Zhang and
                  Giacomo Domeniconi and Vladimir Zolotov and Julian Dolby and
                  Jie Chen and Mihir Choudhury and Lindsey Decker and Veronika Thost and
                  Luca Buratti and Saurabh Pujar and Ulrich Finkler},
  title        = {Project CodeNet: A Large‑Scale AI for Code Dataset for Learning a Diversity of Coding Tasks},
  booktitle    = {NeurIPS Datasets and Benchmarks},
  year         = {2021},
  note         = {arXiv:2105.12655},
  url          = {https://arxiv.org/abs/2105.12655}
}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%[22] -> codemirage [dataset] - preprint
@article{guo2025codemirage,
  title={CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs},
  author={Guo, Hanxi and Cheng, Siyuan and Zhang, Kaiyuan and Shen, Guangyu and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2506.11059},
  year={2025}
}


% Claude‑3.5‑Haiku
@article{anthropic2024model,
  title={Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet},
  author={Anthropic, Sonnet},
  journal={URL https://api. semanticscholar. org/CorpusID},
  volume={273639283},
  year={2024}
}

% DeepSeek‑R1
@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

% DeepSeek‑V3
@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}


@online{google-gemini2-flash-2025,
  author       = {{Google Developers Blog}},
  title        = {Gemini 2.0: Flash, Flash‑Lite and Pro},
  year         = {2025},
  month        = {may},
  day          = {9},
  url          = {https://developers.googleblog.com/en/gemini-2-family-expands/},
  note         = {Accessed: 2025-07-09}
}

%Llama
@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

% Qwen2. 5
@article{hui2024qwen2,
  title={Qwen2. 5-coder technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}




@online{o3mini-openai-2025,
  author       = {{OpenAI}},
  title        = {Introducing OpenAI o3‑mini},
  year         = {2025},
  month        = jan,
  day          = {31},
  url          = {https://openai.com/index/openai-o3-mini/},
  note         = {Includes benchmarks and system card; accessed 2025‑07‑09}
}



@misc{codeparrot-github-code-clean-2022,
  author       = {{CodeParrot}},
  title        = {GitHub Code Clean Dataset},
  howpublished = {\url{https://huggingface.co/datasets/codeparrot/github-code-clean}},
  year         = {2022},
  note         = {Contains ~115M code files across 32 languages; Apache-2.0 license; accessed 2025-07-09}
}


%BLEU
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

%%%%%%%%%%%%%%%%%%%%%%%%

% Pan et al.
@inproceedings{pan2024assessing,
  title={Assessing ai detectors in identifying ai-generated code: Implications for education},
  author={Pan, Wei Hung and Chok, Ming Jie and Wong, Jonathan Leong Shan and Shin, Yung Xin and Poon, Yeong Shian and Yang, Zhou and Chong, Chun Yong and Lo, David and Lim, Mei Kuan},
  booktitle={Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
  pages={1--11},
  year={2024}
}

@misc{quescol2023,
  author       = {{Quescol}},
  title        = {Quescol - A Platform That Provides Previous Year Questions And Answers},
  year         = {2023},
  howpublished = {\url{https://quescol.com/}},
  note         = {Last accessed on Dec 23, 2023}
}


@misc{wikipediaKaggle2023,
  author       = {{Wikipedia contributors}},
  title        = {Kaggle},
  year         = {2023},
  howpublished = {\url{https://en.wikipedia.org/wiki/Kaggle}},
  note         = {Last accessed on Dec 23, 2023}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{konstantinou2024llms,
  title={Do LLMs generate test oracles that capture the actual or the expected program behaviour?},
  author={Konstantinou, Michael and Degiovanni, Renzo and Papadakis, Mike},
  journal={arXiv preprint arXiv:2410.21136},
  year={2024}
}


@article{xu2025kodcode,
      title={KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding}, 
      author={Zhangchen Xu and Yang Liu and Yueqin Yin and Mingyuan Zhou and Radha Poovendran},
      year={2025},
      eprint={2503.02951},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.02951}, 
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{suh2024empirical,
  title={An Empirical Study on Automatically Detecting AI-Generated Source Code: How Far Are We?},
  author={Suh, Hyunjae and Tafreshipour, Mahan and Li, Jiawei and Bhattiprolu, Adithya and Ahmed, Iftekhar},
  journal={arXiv preprint arXiv:2411.04299},
  year={2024}
}
% METODI
%-------------------------------


%[8] DetectGPT4Code -> [detection zero] Yang
@article{yang2023zero,
  title={Zero-shot detection of machine-generated codes},
  author={Yang, Xianjun and Zhang, Kexun and Chen, Haifeng and Petzold, Linda and Wang, William Yang and Cheng, Wei},
  journal={arXiv preprint arXiv:2310.05103},
  year={2023}
}

[6] Paper originale -> [constrastive] Ye
@inproceedings{ye2025uncovering,
  title={Uncovering llm-generated code: A zero-shot synthetic code detector via code rewriting},
  author={Ye, Tong and Du, Yangkai and Ma, Tengfei and Wu, Lingfei and Zhang, Xuhong and Ji, Shouling and Wang, Wenhai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={1},
  pages={968--976},
  year={2025}
}
[21] -> [detection zero] Xu, Zhenyu
@inproceedings{xu2024detecting,
  title={Detecting AI-generated code assignments using perplexity of large language models},
  author={Xu, Zhenyu and Sheng, Victor S},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={38},
  number={21},
  pages={23155--23162},
  year={2024}
}

@article{xu2025distinguishing,
  title={Distinguishing llm-generated from human-written code by contrastive learning},
  author={Xu, Xiaodan and Ni, Chao and Guo, Xinrong and Liu, Shaoxuan and Wang, Xiaoya and Liu, Kui and Yang, Xiaohu},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={34},
  number={4},
  pages={1--31},
  year={2025},
  publisher={ACM New York, NY}
}


@article{guan2024codeip,
  title={Codeip: A grammar-guided multi-bit watermark for large language models of code},
  author={Guan, Batu and Wan, Yao and Bi, Zhangqian and Wang, Zheng and Zhang, Hongyu and Zhou, Pan and Sun, Lichao},
  journal={arXiv preprint arXiv:2404.15639},
  year={2024}
}

@article{kim2025marking,
  title={Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code},
  author={Kim, Jungin and Park, Shinwoo and Han, Yo-Sub},
  journal={arXiv preprint arXiv:2502.18851},
  year={2025}
}

@article{zhang2025robust,
  title={Robust and secure code watermarking for large language models via ml/crypto codesign},
  author={Zhang, Ruisi and Javidnia, Neusha and Sheybani, Nojan and Koushanfar, Farinaz},
  journal={arXiv preprint arXiv:2502.02068},
  year={2025}
}


@article{park2025detection,
  title={Detection of llm-paraphrased code and identification of the responsible llm using coding style features},
  author={Park, Shinwoo and Jin, Hyundong and Cha, Jeong-won and Han, Yo-Sub},
  journal={arXiv preprint arXiv:2502.17749},
  year={2025}
}

@article{park2025detection,
  title={Detection of llm-paraphrased code and identification of the responsible llm using coding style features},
  author={Park, Shinwoo and Jin, Hyundong and Cha, Jeong-won and Han, Yo-Sub},
  journal={arXiv preprint arXiv:2502.17749},
  year={2025}
}

%[13]  Uncovering -> [features] Oedingen
@article{oedingen2024chatgpt,
  title={Chatgpt code detection: Techniques for uncovering the source of code},
  author={Oedingen, Marc and Engelhardt, Raphael C and Denz, Robin and Hammer, Maximilian and Konen, Wolfgang},
  journal={arXiv preprint arXiv:2405.15512},
  year={2024}
}

@article{li2023starcoder,
  title={Starcoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@article{Tunstall2023starchat-alpha,
  author = {Tunstall, Lewis and Lambert, Nathan and Rajani, Nazneen and Beeching, Edward and Le Scao, Teven and von Werra, Leandro and Han, Sheon and Schmid, Philipp and Rush, Alexander},
  title = {Creating a Coding Assistant with StarCoder},
  journal = {Hugging Face Blog},
  year = {2023},
  note = {https://huggingface.co/blog/starchat},
}


@online{openai2025gpt5,
  author    = {OpenAI},
  title     = {{Introducing GPT-5}},
  year      = {2025},
  url       = {https://openai.com/index/introducing-gpt-5/},
  note      = {Accesso: 07 agosto 2025}
}
@article{dora2025hidden,
  title={The hidden risks of LLM-generated web application code: A security-centric evaluation of code generation capabilities in large language models},
  author={Dora, Swaroop and Lunkad, Deven and Aslam, Naziya and Venkatesan, S and Shukla, Sandeep Kumar},
  journal={arXiv preprint arXiv:2504.20612},
  year={2025}
}
% [9] DetectCodeGPT -> [detection zero] Shi
@article{shi2024between,
  title={Between lines of code: Unraveling the distinct patterns of machine and human programmers},
  author={Shi, Yuling and Zhang, Hongyu and Wan, Chengcheng and Gu, Xiaodong},
  journal={arXiv preprint arXiv:2401.06461},
  year={2024}
}

@article{wu2022research,
  title={Research on Code Generation Technology Based on Deep Learning},
  author={Wu, X},
  journal={East China Normal University},
  year={2022}
}



% ALTRI
%-------------------------------

%[3] How far are we -> [paragone] Suh  (propongono anche un metodo)
@article{suh2024empirical,
  title={An Empirical Study on Automatically Detecting AI-Generated Source Code: How Far Are We?},
  author={Suh, Hyunjae and Tafreshipour, Mahan and Li, Jiawei and Bhattiprolu, Adithya and Ahmed, Iftekhar},
  journal={arXiv preprint arXiv:2411.04299},
  year={2024}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{guo2024biscope,
  title={Biscope: Ai-generated text detection by checking memorization of preceding tokens},
  author={Guo, Hanxi and Cheng, Siyuan and Jin, Xiaolong and Zhang, Zhuo and Zhang, Kaiyuan and Tao, Guanhong and Shen, Guangyu and Zhang, Xiangyu},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={104065--104090},
  year={2024}
}





@article{gehrmann2019gltr,
  title={Gltr: Statistical detection and visualization of generated text},
  author={Gehrmann, Sebastian and Strobelt, Hendrik and Rush, Alexander M},
  journal={arXiv preprint arXiv:1906.04043},
  year={2019}
}

@article{lavergne2008detecting,
  title={Detecting Fake Content with Relative Entropy Scoring.},
  author={Lavergne, Thomas and Urvoy, Tanguy and Yvon, Fran{\c{c}}ois},
  journal={Pan},
  volume={8},
  number={27-31},
  pages={4},
  year={2008}
}

@article{hans2024spotting,
  title={Spotting llms with binoculars: Zero-shot detection of machine-generated text},
  author={Hans, Abhimanyu and Schwarzschild, Avi and Cherepanova, Valeriia and Kazemi, Hamid and Saha, Aniruddha and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2401.12070},
  year={2024}
}


@article{liu2024codexembed,
  title={Codexembed: A generalist embedding model family for multiligual and multi-task code retrieval},
  author={Liu, Ye and Meng, Rui and Joty, Shafiq and Savarese, Silvio and Xiong, Caiming and Zhou, Yingbo and Yavuz, Semih},
  journal={arXiv preprint arXiv:2411.12644},
  year={2024}
}


@article{nguyen2024gptsniffer,
  title={GPTSniffer: A CodeBERT-based classifier to detect source code written by ChatGPT},
  author={Nguyen, Phuong T and Di Rocco, Juri and Di Sipio, Claudio and Rubei, Riccardo and Di Ruscio, Davide and Di Penta, Massimiliano},
  journal={Journal of Systems and Software},
  volume={214},
  pages={112059},
  year={2024},
  publisher={Elsevier}
}



@article{wang2023codet5+,
  title={Codet5+: Open code large language models for code understanding and generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi DQ and Li, Junnan and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2305.07922},
  year={2023}
}



@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}



@article{mao2024raidar,
  title={Raidar: generative ai detection via rewriting},
  author={Mao, Chengzhi and Vondrick, Carl and Wang, Hao and Yang, Junfeng},
  journal={arXiv preprint arXiv:2401.12970},
  year={2024}
}

@article{guo2024biscope,
  title={Biscope: Ai-generated text detection by checking memorization of preceding tokens},
  author={Guo, Hanxi and Cheng, Siyuan and Jin, Xiaolong and Zhang, Zhuo and Zhang, Kaiyuan and Tao, Guanhong and Shen, Guangyu and Zhang, Xiangyu},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={104065--104090},
  year={2024}
}



@article{xu2025codevision,
  title={Codevision: Detecting llm-generated code using 2d token probability maps and vision models},
  author={Xu, Zhenyu and Sheng, Victor S},
  journal={arXiv preprint arXiv:2501.03288},
  year={2025}
}



%llama 2
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

%llama 3
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv e-prints},
  pages={arXiv--2407},
  year={2024}
}

%gemma
@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

%minstral
@article{jiang2023mistral,
  title={Mistral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550},
  author={Jiang, AQ and Sablayrolles, A and Mensch, A and Bamford, C and Chaplot, DS and de Las Casas, D and Bressand, F and Lengyel, G and Lample, G and Saulnier, L and others},
  journal={arXiv preprint ARXIV.2310.06825},
  volume={10},
  year={2023}
}


@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}


@article{guo2020graphcodebert,
  title={Graphcodebert: Pre-training code representations with data flow},
  author={Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  journal={arXiv preprint arXiv:2009.08366},
  year={2020}
}




@article{falcon40b,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}