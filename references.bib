@article{base,
  author    = {Ye, Tianyu and Du, Yujia and Ma, Tong and Wu, Lijun and Zhang, Xinyu and Ji, Shuang and Wang, Wei},
  title     = {Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting},
  year      = {2025},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {39},
  number    = {1},
  pages     = {968--976},
  doi       = {10.1609/aaai.v39i1.3208}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION

@article{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and et al.},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and et al.},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{anthropic2023claude,
  title={Claude: The Anthropic Language Model},
  author={Anthropic},
  year={2023},
  howpublished={\url{https://www.anthropic.com/index/introducing-claude}}
}

@article{jiang2023mistral,
  title={Introducing Mistral: A High-Performance Language Model},
  author={Jiang, Jianfeng and et al.},
  journal={Mistral AI},
  year={2023},
  howpublished={\url{https://mistral.ai/news/introducing-mistral-7b/}}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and et al.},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and et al.},
  journal={NeurIPS},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/research/gpt-4}}
}

@inproceedings{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and et al.},
  booktitle={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{roziere2023code,
  title={Code LLaMA: Open Foundation Models for Code},
  author={Roziere, Baptiste and et al.},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{rombach2022high,
  title={High-Resolution Image Synthesis with Latent Diffusion Models},
  author={Rombach, Robin and et al.},
  journal={CVPR},
  year={2022}
}

@misc{google2024veo,
  author    = {Google DeepMind},
  title     = {Veo: Advancing Generative Video Models},
  year      = {2024},
  howpublished = {\url{https://deepmind.google/technologies/veo}}
}

@article{warwick2016turing,
  title={Can Machines Think? A Report on Turing Test Experiments at the Royal Society},
  author={Warwick, Kevin and Shah, Huma},
  journal={Journal of Experimental \& Theoretical Artificial Intelligence},
  volume={28},
  number={6},
  pages={989--1007},
  year={2016}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1.1
@article{weizenbaum1966eliza,
  title={ELIZA—a computer program for the study of natural language communication between man and machine},
  author={Weizenbaum, Joseph},
  journal={Communications of the ACM},
  volume={9},
  number={1},
  pages={36--45},
  year={1966}
}

@article{mikolov2013efficient,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Mikolov, Tomas and et al.},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={GloVe: Global Vectors for Word Representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
  booktitle={EMNLP},
  year={2014}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and et al.},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  year={2019}
}

@inproceedings{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and et al.},
  booktitle={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wang2021codet5,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and et al.},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}

@article{nijkamp2022codegen,
  title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and et al.},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{zeng2022codegeex,
  title={CodeGeeX: A Pre-trained Model for Code Generation with Cross-Programming-Language Evaluation},
  author={Zeng, Guolin and et al.},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{roziere2023code,
  title={Code LLaMA: Open Foundation Models for Code},
  author={Roziere, Baptiste and et al.},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1.1.2
@article{backus1957fortran,
  title={The FORTRAN automatic coding system},
  author={Backus, John and et al.},
  journal={Proceedings of the Western Joint Computer Conference},
  year={1957},
  pages={188--198}
}

@misc{stallman1981emacs,
  title={EMACS: The extensible, customizable self-documenting display editor},
  author={Stallman, Richard M.},
  year={1981},
  howpublished={\url{https://www.gnu.org/software/emacs/}}
}

@misc{xslt1999,
  author={W3C},
  title={XSL Transformations (XSLT) Version 1.0},
  year={1999},
  howpublished={\url{https://www.w3.org/TR/xslt}}
}

@inproceedings{zelle1996learning,
  title={Learning to parse database queries using inductive logic programming},
  author={Zelle, John M. and Mooney, Raymond J.},
  booktitle={AAAI/IAAI, Vol. 2},
  year={1996},
  pages={1050--1055}
}

@inproceedings{mooney1997nlidb,
  title={Learning semantic parsers: An important but under-studied application of machine learning},
  author={Mooney, Raymond J.},
  booktitle={Proceedings of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web},
  year={1997}
}

@misc{jinja2docs,
  title={Jinja2 Documentation},
  author={Pallets Projects},
  year={2005},
  howpublished={\url{https://jinja.palletsprojects.com/}}
}

@misc{makoengine,
  title={Mako Templates for Python},
  author={Mike Bayer},
  year={2006},
  howpublished={\url{https://www.makotemplates.org/}}
}

@inproceedings{yin2017syntactic,
  title={A syntactic neural model for general-purpose code generation},
  author={Yin, Pengcheng and Neubig, Graham},
  booktitle={ACL},
  year={2017}
}

@inproceedings{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and et al.},
  booktitle={arXiv preprint arXiv:2107.03374},
  year={2021}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1.1.3
@article{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={NAACL},
  year={2019}
}

@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and et al.},
  journal={OpenAI blog},
  year={2018}
}

@misc{tabnine2019,
  author={TabNine},
  title={TabNine: Autocomplete AI},
  year={2019},
  howpublished={\url{https://www.tabnine.com/}}
}

@article{husain2019codesearchnet,
  title={CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},
  author={Husain, Hamel and et al.},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}

@article{feng2020codebert,
  title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  author={Feng, Zhangyin and et al.},
  journal={EMNLP},
  year={2020}
}

@article{austin2021mbpp,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and et al.},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{hendrycks2021apps,
  title={Measuring Coding Challenge Competence With APPS},
  author={Hendrycks, Dan and et al.},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}

@article{lu2021codexglue,
  title={CodeXGLUE: A Benchmark Dataset and Open Challenge for Code Intelligence},
  author={Lu, Shuo and et al.},
  journal={arXiv preprint arXiv:2102.04664},
  year={2021}
}

@article{li2022alphacode,
  title={Competition-level Code Generation with AlphaCode},
  author={Li, Yujia and et al.},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}

@article{xu2022polycoder,
  title={PolyCoder: An Open-Source Programming Language Model},
  author={Xu, Alexander and et al.},
  journal={arXiv preprint arXiv:2202.13169},
  year={2022}
}

@article{google2024gemini,
  title={Gemini: Unlocking Multimodal Understanding and Reasoning},
  author={Google DeepMind},
  year={2024},
  howpublished={\url{https://deepmind.google/technologies/gemini/}}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Degradazione del codice 
@article{krishna2023disappearing,
title={The Disappearing Data Problem in ML Training},
author={Krishna, Saurav and Zhang, Yihan and Kıcıman, Emre and Lasecki, Walter S and Singh, Sameer},
journal={arXiv preprint arXiv:2305.00118},
year={2023}
}

% Copiright
@article{zhang2023copyright,
title={Do Foundation Models Know Copyright?},
author={Zhang, Xudong and Raji, Inioluwa Deborah and et al.},
journal={arXiv preprint arXiv:2305.02407},
year={2023}
}

% Security
@inproceedings{perry2022users,
title={Do Users Write More Insecure Code with AI Assistants?},
author={Perry, Alex and Yao, Shixiang and Brown, Nicholas and et al.},
booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
pages={931--948},
year={2022},
organization={IEEE}
}

% studenti
@article{Jost2024LLM,
  author    = {Gregor Jošt and Viktor Taneski and Sašo Karakatič},
  title     = {The Impact of Large Language Models on Programming Education and Student Learning Outcomes},
  journal   = {Applied Sciences},
  year      = {2024},
  volume    = {14},
  number    = {10},
  pages     = {4115},
  doi       = {10.3390/app14104115},
  url       = {https://www.mdpi.com/2076-3417/14/10/4115},
  publisher = {MDPI},
  issn      = {2076-3417}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1.3
@article{mitchell2023detectgpt,
  title={DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khani, Fereshte and Manning, Christopher D. and Finn, Chelsea},
  journal={arXiv preprint arXiv:2301.11305},
  year={2023}
}

@article{krishna2023paraphrasing,
  title={Paraphrasing is All You Need for Detecting AI-Generated Text},
  author={Krishna, Kalpesh and Efrat, Avia and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2306.04636},
  year={2023}
}

%%

@article{ye2023uncovering,
  title={Uncovering LLM-Generated Code via Behavioral and Semantic Analysis},
  author={Ye, Shuyan and et al.},
  journal={arXiv preprint arXiv:2307.05734},
  year={2023}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% PAPER ANALIZZATI %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% DATASET
%-------------------------------

%[1] CoDet_M4 ->[dataset] Orel - preprint
@article{orel2025codet, 
  title={CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings},
  author={Orel, Daniil and Azizov, Dilshod and Nakov, Preslav},
  journal={arXiv preprint arXiv:2503.13733},
  year={2025}
}


%[2] AIGCodeSe -> [dataset] Demirok - preprint
@article{demirok2024aigcodeset,
  title={AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection},
  author={Demirok, Basak and Kutlu, Mucahid},
  journal={arXiv preprint arXiv:2412.16594},
  year={2024}
}


%[22] -> codemirage [dataset] - preprint
@article{guo2025codemirage,
  title={CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs},
  author={Guo, Hanxi and Cheng, Siyuan and Zhang, Kaiyuan and Shen, Guangyu and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2506.11059},
  year={2025}
}








% METODI
%-------------------------------


%[8] DetectGPT4Code -> [detection zero] Yang
@article{yang2023zero,
  title={Zero-shot detection of machine-generated codes},
  author={Yang, Xianjun and Zhang, Kexun and Chen, Haifeng and Petzold, Linda and Wang, William Yang and Cheng, Wei},
  journal={arXiv preprint arXiv:2310.05103},
  year={2023}
}

[6] Paper originale -> [constrastive] Ye
@inproceedings{ye2025uncovering,
  title={Uncovering llm-generated code: A zero-shot synthetic code detector via code rewriting},
  author={Ye, Tong and Du, Yangkai and Ma, Tengfei and Wu, Lingfei and Zhang, Xuhong and Ji, Shouling and Wang, Wenhai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={1},
  pages={968--976},
  year={2025}
}
[21] -> [detection zero] Xu, Zhenyu
@inproceedings{xu2024detecting,
  title={Detecting AI-generated code assignments using perplexity of large language models},
  author={Xu, Zhenyu and Sheng, Victor S},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={38},
  number={21},
  pages={23155--23162},
  year={2024}
}



% [9] DetectCodeGPT -> [detection zero] Shi
@article{shi2024between,
  title={Between lines of code: Unraveling the distinct patterns of machine and human programmers},
  author={Shi, Yuling and Zhang, Hongyu and Wan, Chengcheng and Gu, Xiaodong},
  journal={arXiv preprint arXiv:2401.06461},
  year={2024}
}




% ALTRI
%-------------------------------

%[3] How far are we -> [paragone] Suh  (propongono anche un metodo)
@article{suh2024empirical,
  title={An Empirical Study on Automatically Detecting AI-Generated Source Code: How Far Are We?},
  author={Suh, Hyunjae and Tafreshipour, Mahan and Li, Jiawei and Bhattiprolu, Adithya and Ahmed, Iftekhar},
  journal={arXiv preprint arXiv:2411.04299},
  year={2024}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%