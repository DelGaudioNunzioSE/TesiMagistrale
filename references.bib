@article{base,
  author    = {Ye, Tianyu and Du, Yujia and Ma, Tong and Wu, Lijun and Zhang, Xinyu and Ji, Shuang and Wang, Wei},
  title     = {Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting},
  year      = {2025},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {39},
  number    = {1},
  pages     = {968--976},
  doi       = {10.1609/aaai.v39i1.3208}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
@article{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@misc{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/research/gpt-4}},
  note={Accessed: 2025-06-26}
}

@article{bubeck2023sparks,
  title={Sparks of Artificial General Intelligence: Early Experiments with GPT-4},
  author={Bubeck, SÃ©bastien and Chandrasekaran, Varun and Eldan, Ronen and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}


