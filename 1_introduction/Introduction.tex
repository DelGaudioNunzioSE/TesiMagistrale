\clearpage
\chapter{Introduction}



Generative Artificial Intelligence (AI) is a rapidly evolving subfield of machine learning that has experienced remarkable progress in recent years, particularly due to the advent and widespread adoption of deep learning. At the core of this progress lies the development of the Transformer architecture, introduced by Vaswani et al. in 2017 \cite{vaswani2017attention}, which enabled scalable and efficient modeling of sequential data and laid the foundation for modern language models.

Generative AI encompasses models that are capable of producing new data — including images, audio, or textual content — that imitates human-created material. Among the various deep learning models, Large Language Models (LLMs) have been particularly impactful due to their fluency, versatility, and ease of integration into everyday digital tools. These models, exemplified by GPT-3 \cite{brown2020language}, PaLM \cite{chowdhery2022palm}, and GPT-4 \cite{openai2023gpt4}, are trained on massive textual corpora and possess billions — sometimes hundreds of billions — of parameters.

The ability of LLMs to generate text that is syntactically, semantically, and even stylistically similar to human language has raised fundamental questions about the nature of intelligence. In fact, some studies suggest that these models exhibit early signs of reasoning, abstraction, and planning — characteristics traditionally associated with human cognition. For instance, Bubeck et al. \cite{bubeck2023sparks} analyzed GPT-4 and reported evidence of behaviors that resemble aspects of general intelligence, sparking widespread debate about the emergence of so-called “sparks” of Artificial General Intelligence (AGI).

The success of LLMs has triggered a paradigm shift, transforming them from research prototypes into commercially deployed systems embedded in coding assistants, writing tools, search engines, and chatbots. Their generative capacity, once limited to small-scale language tasks, now extends to complex domains such as legal reasoning, code generation, and scientific hypothesis formation — ushering in new opportunities and challenges across disciplines.

\input{1_introduction/cap/Historical Overview of Code Generation by LLMs}
\input{1_introduction/cap/Motivations Behind LLM-Generated Code Detection}
\input{1_introduction/cap/Challenges in LLM-Generated Code Detection}
\input{1_introduction/cap/Thesis Objectives}