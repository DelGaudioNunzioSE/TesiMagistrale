\section{Evaluation of Existing Methods}
\subsection{Baseline Considerations}
We recall that the work of CodeMirage\cite{guo2025codemirage} was 
not only to provide a dataset, but also to perform 
tests on some of the methods present in the literature.

The methods tested by CodeMirage can be divided into 4 categories:
\begin{enumerate}
    \item \textbf{Zero-shot detectors}: 
    These are methods based on the computation of code-related metrics and do not require any training.
    \newline The advantage of these methods is that they are totally independent of any dataset 
    and therefore “immune” to the \textit{out-of-domain effect}. This characteristic is very useful 
    if the detection method must be per-formant in different domains. 
    \newline Unfortunately, these methods also turn out to be complicated to develop 
    because they require a deep analysis of every type of code generated by an LLM, 
    unless such characteristics exist and are detectable by a non-AI-based method. 
    \newline Moreover, it is essential to identify characteristics that depend intrinsically on the 
    architecture of LLMs and not on their training, otherwise these methods would 
    become even less reliable over time than machine-learning-based methods.

    
    \item \textbf{Embedding-based detectors}: 
    These methods consist of using an encoder-only transformer 
    (or one that can be used as such), trained on code and not 
    fine-tuned for the classification task, paired with a classifier 
    trained on a dataset of LLM and human code.
    \newline These methods always remain inferior to the performance achieved by the next category.
    
    \item \textbf{Fine-tuned detectors}: 
    Fine-tuned methods are like embedding methods, but during training they 
    also train the encoder-only. The difference between the various methods 
    lies only in the choice of the encoder-only model. They seem to be the 
    models that in general achieve the best performance but are also the most 
    affected overall by the out-of-domain effect.
    
    \item \textbf{Pretrained LLM with downstream detector}:
    The last category is a middle ground between the previous ones and seems to 
    suffer overall less from out-of-domain problems. This category statistically 
    analyses the output of an LLM to which the code has been provided via a prompt, 
    such as \texttt{“explain this code:\{code\}”}. Such statistical 
    features are used by a classifier to learn which ones identify 
    human code more and which ones identify 
    LLM code.
\end{enumerate}

\subsubsection{First results}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/CodeMirage/tests.png}
    \caption{CodeMirage's tests}
    \label{fig:CodeMirage-tests}
\end{figure}

The presented methods therefore fall into the previously described categories:
Zero-shot detectors: logRank\cite{gehrmann2019gltr}, entropy\cite{lavergne2008detecting}, 
binoculars\cite{hans2024spotting}.
Embedding-based detectors: Embed-Code, Embed-AST.
Fine-tuned detectors: GPTSniffer\cite{nguyen2024gptsniffer}, CodeT5+\cite{wang2023codet5+}, RoBERTa\cite{liu2019roberta}.
Pretrained LLM with downstream detector: Raidar\cite{mao2024raidar}, Bioscope \cite{guo2024biscope}.


Furthermore, these tests are not complete: 
recall that the CodeMirage dataset includes 
only code coming from GitHub and tends to be 
large. In addition, (more importantly), 
they never mention having removed comments in the code, 
which not infrequently (by analysing the dataset) appear 
to be extremely long and verbose. For this reason it is 
plausible that zero-shot methods designed for natural text 
apparently achieve good results on code as well 
(contrary to what many other works state: 
\textit{ zero-shot text detectors are ineffective in detecting code, 
likely due to the unique statistical properties found in code structures.}
\cite{yang2023zero}
).

It is important to make some clarifications: all the zero-shot 
methods presented were designed to analyze natural text which, 
as previously stated, is a process extremely different from 
analyzing code due to the naturally low next-token perplexity 
(a feature on which almost all zero-shot methods are based). 
The Embed-Code and Embed-AST methods are both based on 
CodeXEmbed-2B\cite{liu2024codexembed}, where in the first case code is provided and 
in the second case the AST obtained from code. GPTSniffer, 
CodeT5+, and RoBERTa are three methods that rely entirely on 
the GPTSniffer\cite{nguyen2024gptsniffer} technique and differ only by the encoder used. 
Finally, both Raidar and Bioscope are methods designed especially 
for natural text, but they seem to achieve good results on code as well.

For all the above reasons, it is justified not to stop at these tests but to pursue more in-depth analyses. In particular, additional methods not considered by CodeMirage and specifically designed for LLM code classification will be examined. Nevertheless, the CodeMirage results should not be ignored.

Key points: the method that appears to deliver the best performance 
is GPTSniffer with CodeT5+. BiScope also merits consideration. 
Rationale: CodeT5+ attains very high TPR at a 10\% 
FPR. However, these tests were run on a split of the 
CodeMirage dataset. Therefore, GPTSniffer with CodeT5+ 
was not evaluated out of domain, which is the main weakness 
of fine-tuned detectors, the category to which GPTSniffer with 
CodeT5+ belongs.

Similarly, BiScope belongs to the pretrained LLM with 
downstream detector category, which has fewer difficulties 
adapting to out-of-domain tests. It also achieves positive 
TPR at an FPR of 10\%. Therefore, GPTSniffer with 
CodeT5+ and BiScope will be considered the baselines 
for subsequent models.




\subsection{Methods to test}
An extensive survey of LLM code-detection methods 
in the literature was conducted. Many works were available 
only as preprints. Several seemingly promising papers could 
not be tested. For example, \cite{xu2025codevision} appears 
to leverage CNNs to analyse images of LLM- and human-written 
code for classification. Unfortunately, neither code nor 
pretrained models were accessible. Consequently, only a small 
set of other methods specifically designed for LLM code detection 
was evaluated.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{BiScope-improved}
Unlike methods that focus only on next-token prediction, 
BISCOPE evaluates text along two axes: next-token prediction 
and previous-token memorization.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/BiScope/Overview.png}
    \caption{BiScope overview}
    \label{fig:BiScope-overview}
\end{figure}

\begin{enumerate}
    \item \textbf{Completion-prompt construction:} an LLM first summarizes the 
    text to obtain global context. The original text is then split in two. 
    The first part, the summary, and a completion request form the prompt fed 
    to a surrogate LLM.
    \item \textbf{Loss computation:} Using the prompt, the surrogate model processes 
    the original text and computes two per-token cross-entropy losses:
    \begin{enumerate}
        \item \textit{Forward Cross-Entropy (FCE)}: how well the model predicts the next token.
        \item \textit{Backward Cross-Entropy (BCE)}: how strongly the model’s logits memorize the immediately preceding token.
    \end{enumerate}
    \item \textbf{Feature extraction:} The text is divided into segments. 
    For each segment, statistics of FCE and BCE are 
    collected: mean, max, min, and standard deviation.

    \item \textbf{Feature classification:} All statistics are concatenated into a 
    single feature vector used to train a binary classifier that decides 
    whether the text is human- or AI-written
\end{enumerate}




Analysing the method, it is seen that many variables exist, such as: 
which LLM to use for completion, into how many sections to split 
the text, whether to give more or less importance to FCE and BCE. 
All these variables were not specified in the CodeMirage paper. 
For this reason, each hyperparameter was set to its default value.
Regarding the \textbf{choice of the LLM}, a different initiative was taken. 
By analysing the LLMs proposed by BISCOPE (and probably also used in the 
CodeMirage tests) on which to evaluate the loss values, it was evident 
that none of those LLMs had been fine-tuned on code:

\begin{table}[ht]
\centering
\begin{tabular}{|p{0.5\textwidth}|p{0.5\textwidth}|}
\hline
\textbf{LLM proposed} & \textbf{Code finetuning?}  \\
\hline
\textbf{Llama-2-7B\cite{touvron2023llama},Llama-2-13B\cite{touvron2023llama}, Llama-3-8B\cite{dubey2024llama}} & No  \\
\hline
\textbf{Gemma-2B\cite{team2024gemma}, Gemma-7B\cite{team2024gemma}} & Code partly present in the training dataset \\
\hline
\textbf{Mistral-7B\cite{jiang2023mistral}} & Code partly present in the training dataset  \\
\hline
\end{tabular}
\caption{Model Comparison and Training on Code}
\label{tab:modelli_codice}
\end{table}

For this reason, a retraining of the classification head was 
performed using \textbf{CodeLlama-7b-hf} \cite{roziere2023code} as the LLM.
It was chosen as the LLM because: it is not among the LLMs that generated 
code in the CodeMirage dataset; it has been fine-tuned on code in several 
languages (Python, C++, Java, PHP, TypeScript, JavaScript, C\#); it has been 
trained to respond to completion requests (required by BiScope); and it is an 
instruct version, as expected by the BiScope framework.

To evaluate the improvements introduced by changing the LLM, in-domain 
tests were conducted on the CodeMirage dataset.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/BiScope/test_noramle_migliorato.png}
    \caption{BiScope test with CodeLlama-7b-hf}
    \label{fig:BiScope-CodeLlama-7b-hf test}
\end{figure}

It is evident that the results are more than excellent. Inspecting the plot, 
it appears that with an FPR of 10\% the model attains an almost perfect TPR. 
It can also be noted that at FPR = 1\% the TPR decreases by a non-negligible 
amount. This means that, under these conditions, the model is usable only by 
accepting an error probability of about 10\% or slightly less.

\subsubsection{GPTSniffer-CodeT5+ multilingual}
This method is based on another work, GPTSniffer\cite{nguyen2024gptsniffer}, 
with the only difference 
of using a different encoder-only model. This also shows how the same methods 
can obtain extremely different results (see the TPR results at FPR = 1\% 
reported by CodeMirage \ref{fig:CodeMirage-tests}) 
without changing the methodological aspect but only by 
changing the transformer used. This should not be surprising, since much of the 
work is carried out by these networks, but it also shows how the results of the 
same method can vary greatly when a different transformer is used.

The GPTSniffer technique is based on two principles:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/gptsniffer/system components.png}
    \caption{GPTSniffer system}
    \label{fig:GPTSniffer system}
\end{figure}
\begin{enumerate}
    \item Train the transformer and the classification head on code style, 
    not on the code itself. 
    They seek to obtain this result by applying heavy preprocessing that:   
    \begin{enumerate}
        \item Removal of all imports
        \item Removal of comments
        \item Removal of formatting characters
        \item Replacement of class name
    \end{enumerate}
    \item Fine-tune the encoder-only module to obtain the best embedding 
    representation to be classified by the head.
\end{enumerate}

Unlike CodeMirage, to carry out these tests a \textbf{preprocessing 
version applicable to many programming languages} (all those present in 
the CodeMirage dataset) was developed. In fact, GPTSniffer focuses only on 
Python, which would be a problem both during training and during testing on 
a multilingual dataset.

To evaluate the improvements introduced by changing the LLM, in-domain tests 
were conducted on the CodeMirage dataset.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/CodeT5/default.png}
    \caption{CodeT5 tests with multilingual preprocessing.}
    \label{fig:CodeT5 tests with multilingual preprocessing}
\end{figure}

Excellent performance is observed here as well. Compared with BiScope 
with CodeLlama, performance across languages remains much more variable 
(higher standard deviation). Nevertheless, very good TPR is obtained at FPR = 1\%, 
a result that is interesting and useful for scenarios where avoiding unjust 
accusations is required.



\subsubsection{UncoveringLLM}
This method was one of the first proposed for detection specifically on code. 
After recognizing that the problem on code was profoundly more complex than on 
human code, a new technique was implemented. Unfortunately, it should be noted 
that the provided repository was wholly insufficient. For this reason, the entire 
technique they proposed was reproduced from scratch.
The technique is based on a key observation: when an LLM rewrites code it previously 
generated, the differences between the two versions are minimal. Conversely, if the 
LLM rewrites code written by a human, the differences are much more significant. 
Leveraging this intuition, the detector operates in three fundamental steps:

\begin{enumerate}
\item \textbf{Rewrite the code:} An LLM is used to rewrite the code snippet under analysis. 
The model is instructed to explain the code’s functionality and rewrite it, a 
method based on chain-of-thought prompting.

\item \textbf{Measure similarity:} A code-similarity model, such as 
GraphCodeBERT\cite{guo2020graphcodebert} 
(trained with self-supervised contrastive learning), computes a similarity 
score between the original code and its rewritten version. The framework is 
flexible and not tied to a specific similarity model.

\item \textbf{Compute the detection score:} The rewriting process is repeated multiple times 
(m times), and the final similarity score is the mean of the scores from each 
rewrite. If the score exceeds a threshold, the code is classified as LLM-generated. 
According to experiments, only four rewrites are sufficient to achieve excellent results.
\end{enumerate}

The method, although interesting, presents a major problem in use: 
at least two rewrites are required for each code snippet, greatly 
lengthening both the training and inference phases. For this reason, 
despite the work recommending 4 rewrites for higher performance, the tests 
will use two rewrites, which according to the original paper’s tests does not 
reduce the F1-score by more than 0.1.

To use this method, the previously described framework~\ref{section:Test framework} 
was therefore used (with slight modifications) to generate code rewrites on a 
dataset. In this way it was possible to run tests on the various available datasets.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/UncoveringLLM/framework.jpg}
    \caption{UncoveringLLM Framework}
    \label{fig:UncoveringLLM Framework}
\end{figure}




\subsubsection{LLMPPL}
The work “Detecting AI-Generated Code Assignments Using Perplexity 
of Large Language Models” proposes a zero-shot detection method based 
on perplexity. However, from their official repository on 
\href{https://github.com/Arrtourz/llmppl}{GitHub}.

Quickly repeating how the perplexity score is computed in their library:
\begin{enumerate}
    \item \textbf{Tokenization} \\
    A text sequence is converted into token IDs:
    \[
        x_1, x_2, \dots, x_N.
    \]

    \item \textbf{Autoregressive prediction} \\
    The model produces logits for each position $t$ given the prefix:
    \[
        Z_t = f_\theta(x_1, \dots, x_t), \quad Z_t \in R^V,
    \]
    where $V$ is the vocabulary size.

    \item \textbf{Token probabilities} \\
    Apply the softmax function to obtain probabilities:
    \[
        p_\theta(v \mid x_{\leq t}) = \frac{\exp(Z_t[v])}{\sum_{u=1}^V \exp(Z_t[u])}.
    \]

    \item \textbf{Shifted targets} \\
    For causal language modeling, the target at step $t$ is the next token:
    \[
        y_t = x_{t+1}, \quad t = 1, \dots, N-1.
    \]

    \item \textbf{Negative log-likelihood per token} \\
    \[
        \mathrm{NLL}_t = - \log p_\theta(y_t \mid x_{\leq t}).
    \]

    \item \textbf{Average loss (cross-entropy in nats)} \\
    \[
        \mathrm{loss} = \frac{1}{N-1} \sum_{t=1}^{N-1} \mathrm{NLL}_t
        = -\frac{1}{N-1} \sum_{t=1}^{N-1} \log p_\theta(x_{t+1} \mid x_{\leq t}).
    \]

    \item \textbf{Perplexity} \\
    \[
        \mathrm{PPL} = \exp(\mathrm{loss}).
    \]
\end{enumerate}

In order to select the best perplexity threshold, 
a CoDet-M4\ref{section:CoDet-M4} sub-dataset was used, and the threshold that 
maximized AUROC was set.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/LLMPPL/AUC.png}
    \caption{MAX AUROC used to select the threshold.}
    \label{fig:AUROC used to select the threshold.}
\end{figure}












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CodeMirage test}
The first tests were conducted on CodeMirage with a very important 
precaution. The non-zero-shot models were trained on a different dataset 
(CoDet-M4\ref{section:CoDet-M4}, with clean code, i.e., code from which every 
comment was removed). Moreover, in all analysed code, all comments were 
removed across languages to ensure that no method relies on natural 
language for detection.

\subsubsection{BiScope-improved}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/BiScope/Codemirage.png}
    \caption{BiScope-improved-CodeMirage test.}
    \label{fig:BiScope-improved-CodeMirage test}
\end{figure}

\subsubsection{GPTSniffer-CodeT5+ multilingual}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/CodeT5/CodeMirage.png}
    \caption{GPTSniffer-CodeT5+ multilingual-CodeMirage test.}
    \label{fig:GPTSniffer-CodeT5+ multilingual-CodeMirage test}
\end{figure}


\subsubsection{UncoveringLLM}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/UncoveringLLM/CodeMirage.png}
    \caption{UncoveringLLM-CodeMirage test.}
    \label{fig:UncoveringLLM-CodeMirage test}
\end{figure}



\subsubsection{LLMPL}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/LLMPPL/CodeMirage.png}
    \caption{LLMPPL/-CodeMirage test.}
    \label{fig:LLMPPL-CodeMirage test}
\end{figure}




%%%%
\subsection{AIGCode test}

\subsubsection{BiScope-improved}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/BiScope/AIG.png}
    \caption{BiScope-improved-CodeMirage test.}
    \label{fig:BiScope-improved-CodeMirage test}
\end{figure}

\subsubsection{GPTSniffer-CodeT5+ multilingual}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/CodeT5/AIG.png}
    \caption{GPTSniffer-CodeT5+ multilingual-CodeMirage test.}
    \label{fig:GPTSniffer-CodeT5+ multilingual-CodeMirage test}
\end{figure}


\subsubsection{UncoveringLLM}




\subsubsection{LLMPL}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/LLMPPL/AIG.png}
    \caption{LLMPPL/-CodeMirage test.}
    \label{fig:LLMPPL-CodeMirage test}
\end{figure}

