\section{Evaluation of Existing Methods}
\subsection{Baseline Considerations}
We recall that the work of CodeMirage\cite{guo2025codemirage} was 
not only to provide a dataset, but also to perform 
tests on some of the methods present in the literature.

The methods tested by CodeMirage can be divided into 4 categories:
\begin{enumerate}
    \item \textbf{Zero-shot detectors}: 
    These are methods based on the computation of code-related metrics and do not require any training.
    \newline The advantage of these methods is that they are totally independent of any dataset 
    and therefore “immune” to the \textit{out-of-domain effect}. This characteristic is very useful 
    if the detection method must be per-formant in different domains. 
    \newline Unfortunately, these methods also turn out to be complicated to develop 
    because they require a deep analysis of every type of code generated by an LLM, 
    unless such characteristics exist and are detectable by a non-AI-based method. 
    \newline Moreover, it is essential to identify characteristics that depend intrinsically on the 
    architecture of LLMs and not on their training, otherwise these methods would 
    become even less reliable over time than machine-learning-based methods.

    
    \item \textbf{Embedding-based detectors}: 
    These methods consist of using an encoder-only transformer 
    (or one that can be used as such), trained on code and not 
    fine-tuned for the classification task, paired with a classifier 
    trained on a dataset of LLM and human code.
    \newline These methods always remain inferior to the performance achieved by the next category.
    
    \item \textbf{Fine-tuned detectors}: 
    Fine-tuned methods are like embedding methods, but during training they 
    also train the encoder-only. The difference between the various methods 
    lies only in the choice of the encoder-only model. They seem to be the 
    models that in general achieve the best performance but are also the most 
    affected overall by the out-of-domain effect.
    
    \item \textbf{Pretrained LLM with downstream detector}:
    The last category is a middle ground between the previous ones and seems to 
    suffer overall less from out-of-domain problems. This category statistically 
    analyses the output of an LLM to which the code has been provided via a prompt, 
    such as \texttt{“explain this code:\{code\}”}. Such statistical 
    features are used by a classifier to learn which ones identify 
    human code more and which ones identify 
    LLM code.
\end{enumerate}

\subsubsection{First results}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/CodeMirage/tests.png}
    \caption{CodeMirage's tests}
    \label{fig:CodeMirage-tests}
\end{figure}

The presented methods therefore fall into the previously described categories:
Zero-shot detectors: logRank\cite{gehrmann2019gltr}, entropy\cite{lavergne2008detecting}, 
binoculars\cite{hans2024spotting}.
Embedding-based detectors: Embed-Code, Embed-AST.
Fine-tuned detectors: GPTSniffer\cite{nguyen2024gptsniffer}, CodeT5+\cite{wang2023codet5+}, RoBERTa\cite{liu2019roberta}.
Pretrained LLM with downstream detector: Raidar\cite{mao2024raidar}, Bioscope \cite{guo2024biscope}.


Furthermore, these tests are not complete: 
recall that the CodeMirage dataset includes 
only code coming from GitHub and tends to be 
large. In addition, (more importantly), 
they never mention having removed comments in the code, 
which not infrequently (by analysing the dataset) appear 
to be extremely long and verbose. For this reason it is 
plausible that zero-shot methods designed for natural text 
apparently achieve good results on code as well 
(contrary to what many other works state: 
\textit{ zero-shot text detectors are ineffective in detecting code, 
likely due to the unique statistical properties found in code structures.}
\cite{yang2023zero}
).

It is important to make some clarifications: all the zero-shot 
methods presented were designed to analyze natural text which, 
as previously stated, is a process extremely different from 
analyzing code due to the naturally low next-token perplexity 
(a feature on which almost all zero-shot methods are based). 
The Embed-Code and Embed-AST methods are both based on 
CodeXEmbed-2B\cite{liu2024codexembed}, where in the first case code is provided and 
in the second case the AST obtained from code. GPTSniffer, 
CodeT5+, and RoBERTa are three methods that rely entirely on 
the GPTSniffer\cite{nguyen2024gptsniffer} technique and differ only by the encoder used. 
Finally, both Raidar and Bioscope are methods designed especially 
for natural text, but they seem to achieve good results on code as well.

For all the above reasons, it is justified not to stop at these tests but to pursue more in-depth analyses. In particular, additional methods not considered by CodeMirage and specifically designed for LLM code classification will be examined. Nevertheless, the CodeMirage results should not be ignored.

Key points: the method that appears to deliver the best performance 
is GPTSniffer with CodeT5+. BiScope also merits consideration. 
Rationale: CodeT5+ attains very high TPR at a 10\% 
FPR. However, these tests were run on a split of the 
CodeMirage dataset. Therefore, GPTSniffer with CodeT5+ 
was not evaluated out of domain, which is the main weakness 
of fine-tuned detectors, the category to which GPTSniffer with 
CodeT5+ belongs.

Similarly, BiScope belongs to the pretrained LLM with 
downstream detector category, which has fewer difficulties 
adapting to out-of-domain tests. It also achieves positive 
TPR at an FPR of 10\%. Therefore, GPTSniffer with 
CodeT5+ and BiScope will be considered the baselines 
for subsequent models.




\subsection{Methods to test}
An extensive survey of LLM code-detection methods 
in the literature was conducted. Many works were available 
only as preprints. Several seemingly promising papers could 
not be tested. For example, \cite{xu2025codevision} appears 
to leverage CNNs to analyse images of LLM- and human-written 
code for classification. Unfortunately, neither code nor 
pretrained models were accessible. Consequently, only a small 
set of other methods specifically designed for LLM code detection 
was evaluated.

In order to have a good comparison among the different methods, 
the tests will also be performed on the two baseline methods 
obtained from the CodeMirage work.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{BiScope}
Unlike methods that focus only on next-token prediction, 
BISCOPE evaluates text along two axes: next-token prediction 
and previous-token memorization.
\begin{enumerate}
    \item \textbf{Completion-prompt construction:} an LLM first summarizes the 
    text to obtain global context. The original text is then split in two. 
    The first part, the summary, and a completion request form the prompt fed 
    to a surrogate LLM.
    \item \textbf{Loss computation:} Using the prompt, the surrogate model processes 
    the original text and computes two per-token cross-entropy losses:
    \begin{enumerate}
        \item \textit{Forward Cross-Entropy (FCE)}: how well the model predicts the next token.
        \item \textit{Backward Cross-Entropy (BCE)}: how strongly the model’s logits memorize the immediately preceding token.
    \end{enumerate}
    \item \textbf{Feature extraction:} The text is divided into segments. 
    For each segment, statistics of FCE and BCE are 
    collected: mean, max, min, and standard deviation.

    \item \textbf{Feature classification:} All statistics are concatenated into a 
    single feature vector used to train a binary classifier that decides 
    whether the text is human- or AI-written
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/BiScope/Overview.png}
    \caption{BiScope overview}
    \label{fig:BiScope-overview}
\end{figure}


Analyzing the method, it is seen that many variables exist, such as: 
which LLM to use for completion, into how many sections to split 
the text, whether to give more or less importance to FCE and BCE. 
All these variables were not specified in the CodeMirage paper. 
For this reason, each hyperparameter was set to its default value.
Regarding the choice of the LLM, the LLM most dissimilar from the 
data present in the CodeMirage dataset and at the same time extremely 
lightweight was selected: CodeLlama-7b-hf. The tests were then repeated 
on the CodeMirage dataset by analyzing both original code (with comments) 
and code from which every form of comment was removed.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/BiScope/test normale.png}
    \caption{BiScope test}
    \label{fig:BiScope-default test}
\end{figure}

It is evident that the results are not consistent with those reported by 
CodeMirage. Unfortunately, since the CodeMirage test code is not available, 
it was not possible to assess the cause of the discrepancy. In any case, it 
should be considered that a method specifically designed for natural text 
detection can detect code with a TPR of 0.4 at an FPR of 1\%. 
It is nevertheless better than standard zero-shot methods designed 
for natural text.


\subsubsection{GPTSniffer-CodeT5+}
This method is based on another work, GPTSniffer\cite{nguyen2024gptsniffer}, 
with the only difference 
of using a different encoder-only model. This also shows how the same methods 
can obtain extremely different results (see the TPR results at FPR = 1\% 
reported by CodeMirage \ref{fig:BiScope-default test}) 
without changing the methodological aspect but only by 
changing the transformer used. This should not be surprising, since much of the 
work is carried out by these networks, but it also shows how the results of the 
same method can vary greatly when a different transformer is used.

The GPTSniffer technique is based on two principles:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/gptsniffer/system components.png}
    \caption{GPTSniffer system}
    \label{fig:GPTSniffer system}
\end{figure}
\begin{enumerate}
    \item Train the transformer and the classification head on code style, 
    not on the code itself, via preprocessing.  
    \begin{enumerate}
        \item Removal of all imports
        \item Removal of comments
        \item Removal of formatting characters
        \item Replacement of class name
    \end{enumerate}
    \item Fine-tune the encoder-only module to obtain the best embedding 
    representation to be classified by the head.
\end{enumerate}
Although not specified in the CodeMirage work, care was 
taken to develop a version of GPTSniffer preprocessing 
functions that is general for all languages present in the dataset, 
and not only Python \textit{(as GPTSniffer)}.