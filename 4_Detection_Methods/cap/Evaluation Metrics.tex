\section{Evaluation Metrics}
Let us recall that the ultimate goal is to build a 
binary classifier capable of determining whether a piece 
of source code was written by a human or generated by a 
Large Language Model (LLM). For binary classifiers, 
\textbf{accuracy} is typically the default evaluation 
metric, unless one classification outcome, such as true 
positives or false positives, is considered more critical 
than the other. 

Given that this classifier can be used 
in scenarios where a programmer could be 'accused' of 
not having authored the code themselves, it is essential 
that such claims be made with a high degree of confidence. 
While it is certainly desirable to allow end users to 
configure the decision threshold, the default setting 
should prioritize a \textbf{low FPR} (false positive rate) 
to prevent unjust accusations.
\begin{quote}
    “In previous experiments, we mainly use F1 score, 
    which is a threshold-dependent measure that balances 
    precision and recall, but F1 can be misleading in 
    real-world detection tasks. As it gives equal weight 
    to false positives and false negatives […] it often 
    fails to reflect performance in imbalanced settings or 
    under strict false-alarm constraints. By contrast, 
    reporting the true positive rate at low false-positive 
    rates directly measures how many genuine positives the 
    model catches when false alarms must be kept to a minimum.”
    \cite{guo2025codemirage}.
\end{quote}


Given these considerations, the most interesting metrics for evaluating 
the difference between different detection methods are the 
TPR (True Positive Rate) at a fixed FPR (False Positive Rate), 
as well as the F1-Score, which allows for a general comparison 
between multiple detection methods in cases where the FPR is not a 
critical parameter for the method's application domain.

Therefore, the following will be evaluated: F1-Score, TPR@FPR = 10\%, and TPR@FPR = 1\%.


\section{Evaluation Methods}
Analyzing various works that propose detection methods, several 
"adversarial scenarios" have emerged, which can be considered more or less challenging:

\begin{enumerate}
\item \textbf{Out-of-domain:} All machine learning-based methods struggle significantly 
when tested on datasets different from the training dataset. It is therefore 
interesting to evaluate the performance loss between different datasets, considering:
    \begin{enumerate}
    \item \textbf{Type of code} present in the dataset \textit{(length, paradigm, language, etc)}
    \item \textbf{LLMs} in the dataset different from the LLM used during training.
    \end{enumerate}
\item \textbf{Comment removal:} It would be extremely easy for a user to remove all 
comments within the code. The reason is clear: comments are essentially 
natural language, and it is likely that a detection method could exploit 
comments to detect LLM-generated text.

\item \textbf{Indentation removal:} Optional indentation could be another challenge, 
as removing or modifying the indentation might confuse detection methods 
that rely on structural features of the code.

\item \textbf{Paraphrasing:} It would not be difficult for a user to change variable and 
function names, perhaps calling them with more standard names. 
Changing names becomes another realistic and difficult scenario for detection methods.
\end{enumerate}

The probable reason why the \textbf{Out-of-domain scenario} is so challenging 
may be related to the following statement:
\textit{"Every programmer develops a unique coding style, shaped by their 
routines and habits. Similarly, generative models, like experienced 
programmers, exhibit distinctive coding patterns influenced by the 
biases present in their training data. Therefore, LLMs can be viewed 
as programmers with consistent coding styles shaped by these inherent biases."}
\cite{ye2023uncovering}
Thus, it can be thought that the goal of a detection method for 
LLM-generated code is simply to identify a specific coding style 
(whether from a particular programmer or from an LLM). Fortunately, 
we can hope that multiple LLMs share common traits, because:
\textit{"Since LLMs are trained on vast corpora of data from the Internet, 
their training data likely exhibit significant similarities, leading 
to similar behaviors across models."}\cite{guo2024biscope}
\\\\
That said, some tests are more complex and less realistic 
than others. For example, out-of-domain tests are more relevant 
in real-world settings than indentation removal. For these reasons, 
the focus will be on the most important metrics 
(F1-score, TPR@FPR = 10\%, and TPR@FPR = 1\%) 
on in-domain, \textbf{out-of-domain}, and \textbf{comment-removal} tests.

It is explained from now that all test plots will include these three selected metrics 
(F1-score, TPR@FPR = 10\%, TPR@FPR = 1\%). In particular, the values of F1, TPR@FPR = 
10\%, and TPR@FPR = 1\% averaged across languages will be reported. 
At the same time, an error bar computed via the standard deviation on 
the average values will be shown. Variation across programming languages, 
not across LLMs, was chosen for reasons of informational utility. If the 
joint variability across languages and generating LLMs were evaluated, the 
source of uncertainty would be difficult to understand. Moreover, uncertainty 
due to differences between LLMs is less significant (because more tied to training 
data) than that due to the programming language. If variation across LLMs is of 
interest, it is more useful to display the performance that the model obtains on a 
specific LLM.

This policy was not applied solely to the metrics reported by the 
CodeMirage paper (figure \ref{fig:CodeMirage-tests}), which appear to have been evaluated both over LLM and over language.

To allow anyone to run tests for any preferred LLM, 
a graphical interface was created that allows easy testing 
of the methods presented in this work. This part will be detailed in Section 6.